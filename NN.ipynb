{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_data = pd.read_excel(\"../UNT-Thermography-24/fused_silica_data/fused_silica_exact_train.xlsx\")\n",
    "\n",
    "exact_df = pd.DataFrame(exact_data)\n",
    "\n",
    "X = exact_df[exact_df.columns[11:]]\n",
    "\n",
    "Y = exact_df[exact_df.columns[0:11]]\n",
    "\n",
    "print(f\"The original data set has {exact_df.shape[0]} rows, and {exact_df.shape[1]} columns. \"\n",
    "      f\"We want to predict {target.shape[1]} output variables from {features.shape[1]} features\")\n",
    "\n",
    "exact_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize features\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goal: Using keras to implement a feed-forward neural network to predict temperatures of each layer given wavelength spectrum\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled_df, Y, train_size = .8, random_state = 1337)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, train_size=0.8, random_state = 1337)\n",
    "\n",
    "print(\n",
    "    f\"Using {len(Y_train)} samples for training, \"\n",
    "    f\"and {len(Y_test)} for testing \"\n",
    "    f\"from {len(X_test) + len(X_train)} total samples.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shape\n",
    "input_layer = keras.Input(shape=(X_train.shape[1],))\n",
    "\n",
    "#Define the model (add more layers? increase nodes?)\n",
    "exact_model = keras.Sequential([input_layer, #input_layer --> informs input shape\n",
    "    layers.Dense(256, activation=\"relu\"), # first hidden layer\n",
    "    layers.Dense(128, activation=\"relu\"), # second hidden layer\n",
    "    # layers.Dense(20, activation=\"relu\"), # third hidden layer\n",
    "    # layers.Dense(20, activation=\"relu\"), # fourth hidden layer\n",
    "    layers.Dense(11)  # Output layer with 11 neurons for each temperature target\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "# Instantiate optimizer with adjustable learning rate\n",
    "# observed: sweet spot .02 > x > .001\n",
    "Adam1 = Adam(learning_rate = 0.009)\n",
    "\n",
    "# Leverage both MSE and MAE (experimental)\n",
    "# test different optimizers: SGD (*stochastic gradient descent: better for large data sets)\n",
    "exact_model.compile(loss=\"mse\", optimizer=Adam1, metrics=[\"mae\"])  # Loss: MSE, Optimizer: Adam w/ MAE\n",
    "\n",
    "# Train the model\n",
    "# Thoughts: Overfit then implement regularization\n",
    "# play around with batch size: batch size for larger datasets more likely to improve its performance\n",
    "# Dima went up to 60 epochs for GaN_exact: later 600 -> imrpovement plateaus around 120 epochs\n",
    "# plot loss over epochs\n",
    "history = exact_model.fit(X_train, Y_train, epochs=15, validation_data = [X_val, Y_val])\n",
    "\n",
    "#View Training Error\n",
    "print(f\"train_MSE: {history.history[\"loss\"]}\")\n",
    "print(f\"train_MAE: {history.history.get(\"mae\", None)}\")\n",
    "\n",
    "# Evaluate the model on the validation set (separate call)\n",
    "validation_loss, validation_accuracy = exact_model.evaluate(X_val, Y_val)\n",
    "\n",
    "print(f\"val_MSE: {validation_loss}, val_MAE: {validation_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on unseen data\n",
    "loss, mae = exact_model.evaluate(X_test, Y_test)\n",
    "print(\"test_MSE:\", loss)\n",
    "print(\"test_MAE:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "#plot predictions against actual (scatter plot to see if similar)\n",
    "#m nodes by n layers: want m*n to be less than number of data points\n",
    "#per layer residual\n",
    "#avg (above) to get overall residual\n",
    "#plot error over epochs\n",
    "#increase layers: implement loop\n",
    "#change activation function to hyperbolic tangent: (theoretical difference: ReLu 0 and 1, tangent maintains positive and negative (if normalized data is positive and negative, it is possible tanegent would work better (consider what might happen if all data is positive)))\n",
    "# 20 layers 250 nodes, batch size 15, epochs 220\n",
    "#learning rate vs momenta/momento?? may reduce epochs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
