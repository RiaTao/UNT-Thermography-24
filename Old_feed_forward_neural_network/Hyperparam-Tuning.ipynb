{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, LeakyReLU\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.initializers import he_normal\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, PolynomialFeatures, StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Load .mat file\n",
    "mat = scipy.io.loadmat('../UNT-Thermography-24/fused_silica_data/data_GaN_8_15micron_1.mat')\n",
    "\n",
    "# Extract and reshape data\n",
    "Temp_all = mat.get('T_GaN_random')\n",
    "temp_line = Temp_all.reshape(2000, 50)\n",
    "\n",
    "Spectra_all = mat.get('Spectrum')\n",
    "spectra_line = Spectra_all.reshape(2000, 355)\n",
    "\n",
    "# Create pandas DataFrame\n",
    "pd_spectra = pd.DataFrame(spectra_line)\n",
    "pd_temp = pd.DataFrame(temp_line)\n",
    "\n",
    "# Select the data\n",
    "X = pd_spectra.iloc[0:2000]\n",
    "Y = pd_temp.iloc[0:2000]\n",
    "\n",
    "# Scale the features using RobustScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Create polynomial features\n",
    "# poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# X_poly = poly.fit_transform(X_scaled)\n",
    "\n",
    "# # Apply PCA for dimensionality reduction\n",
    "# pca = PCA(n_components=50)  # Adjust n_components based on variance explained\n",
    "# X_pca = pca.fit_transform(X_poly)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=1337)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1337)\n",
    "\n",
    "X_train_normal = normalize(X_train)\n",
    "X_val_normal = normalize(X_val)\n",
    "X_test_normal = normalize(X_test)\n",
    "\n",
    "# Create model function\n",
    "def create_model(optimizer='adam', activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train_normal.shape[1],)))\n",
    "    for _ in range(30): \n",
    "        model.add(Dense(30))\n",
    "        if activation == 'leaky_relu':\n",
    "            model.add(LeakyReLU(alpha=0.1))\n",
    "        else:\n",
    "            model.add(tf.keras.layers.Activation(activation))\n",
    "        # model.add(BatchNormalization())\n",
    "        # model.add(Dropout(0.2))\n",
    "    model.add(Dense(50))\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Define KerasRegressor\n",
    "model = KerasRegressor(model=create_model, batch_size=32, epochs=100, verbose=0)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "                #   , 'adamw', 'adadelta', 'adagrad', 'adamax', 'nadam'],\n",
    "    'activation': ['relu', 'tanh','leaky_relu'],\n",
    "                #    'elu', 'sigmoid']\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [50, 100, 200]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best combination of hyperparameters\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train_normal, y_train)\n",
    "\n",
    "# Display the best hyperparameters and corresponding score\n",
    "print(f\"Best Parameters: {grid_result.best_params_}\")\n",
    "print(f\"Best Score: {grid_result.best_score_}\")\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_params = grid_result.best_params_\n",
    "best_model = create_model(optimizer=best_params['optimizer'], activation=best_params['activation'])\n",
    "\n",
    "history = best_model.fit(X_train_normal, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'], validation_data=(X_val_normal, y_val), verbose=1)\n",
    "\n",
    "# Save the model\n",
    "# best_model.save('improved_temperature_prediction_model_v2.keras')\n",
    "\n",
    "# Print training metrics\n",
    "print(f\"train_MSE: {history.history['loss'][-1]}\")\n",
    "print(f\"train_MAE: {history.history['mae'][-1]}\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "validation_loss, validation_mae = best_model.evaluate(X_val_normal, y_val)\n",
    "print(f\"val_MSE: {validation_loss}, val_MAE: {validation_mae}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_mae = best_model.evaluate(X_test_normal, y_test)\n",
    "print(f\"test_MSE: {test_loss}, test_MAE: {test_mae}\")\n",
    "\n",
    "# Add test metrics to the history for plotting\n",
    "history.history['test_loss'] = [test_loss] * len(history.history['loss'])\n",
    "history.history['test_mae'] = [test_mae] * len(history.history['mae'])\n",
    "\n",
    "# Plot training & validation loss and mae values\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"loss\"], label='Training Loss')\n",
    "plt.plot(history.history[\"val_loss\"], label='Validation Loss')\n",
    "plt.plot(history.history[\"test_loss\"], label='Test Loss')\n",
    "plt.title('Training, Validation, and Testing Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot training and validation MAE\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"mae\"], label=\"Training MAE\")\n",
    "plt.plot(history.history[\"val_mae\"], label=\"Validation MAE\")\n",
    "plt.plot(history.history[\"test_mae\"], label=\"Test MAE\")\n",
    "plt.title('Training, Validation, and Testing MAE over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Input(shape=(X_train_normal.shape[1],)))\n",
    "\n",
    "# for i in range(30): \n",
    "#     model.add(Dense(30, activation=\"relu\"))\n",
    "#     model.add(Dropout(0.3))\n",
    "\n",
    "# model.add(Dense(50))\n",
    "\n",
    "# model.compile(optimizer=\"rmsprop\", loss='mse', metrics=['mae'])\n",
    "\n",
    "# history = model.fit(X_train_normal, y_train, epochs=150, batch_size=16, validation_data=(X_val_normal, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
