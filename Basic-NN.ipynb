{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data set has 10000 rows, and 77 columns. We want to predict 11 output variables from 66 features\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer 1</th>\n",
       "      <th>layer 2</th>\n",
       "      <th>layer 3</th>\n",
       "      <th>layer 4</th>\n",
       "      <th>layer 5</th>\n",
       "      <th>layer 6</th>\n",
       "      <th>layer 7</th>\n",
       "      <th>layer 8</th>\n",
       "      <th>layer 9</th>\n",
       "      <th>layer 10</th>\n",
       "      <th>...</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>312.355087</td>\n",
       "      <td>301.649641</td>\n",
       "      <td>321.270850</td>\n",
       "      <td>324.903956</td>\n",
       "      <td>329.245296</td>\n",
       "      <td>337.694194</td>\n",
       "      <td>333.776774</td>\n",
       "      <td>333.750946</td>\n",
       "      <td>331.605971</td>\n",
       "      <td>333.828275</td>\n",
       "      <td>...</td>\n",
       "      <td>2.378753e-12</td>\n",
       "      <td>2.416740e-12</td>\n",
       "      <td>2.452565e-12</td>\n",
       "      <td>2.482976e-12</td>\n",
       "      <td>2.505710e-12</td>\n",
       "      <td>2.514345e-12</td>\n",
       "      <td>2.497372e-12</td>\n",
       "      <td>2.443515e-12</td>\n",
       "      <td>2.360022e-12</td>\n",
       "      <td>2.283099e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>308.045992</td>\n",
       "      <td>314.390077</td>\n",
       "      <td>319.612686</td>\n",
       "      <td>324.021871</td>\n",
       "      <td>328.286597</td>\n",
       "      <td>324.268581</td>\n",
       "      <td>318.045216</td>\n",
       "      <td>310.205743</td>\n",
       "      <td>316.395558</td>\n",
       "      <td>315.206667</td>\n",
       "      <td>...</td>\n",
       "      <td>4.690686e-12</td>\n",
       "      <td>4.758857e-12</td>\n",
       "      <td>4.814785e-12</td>\n",
       "      <td>4.855500e-12</td>\n",
       "      <td>4.879557e-12</td>\n",
       "      <td>4.876002e-12</td>\n",
       "      <td>4.823191e-12</td>\n",
       "      <td>4.700047e-12</td>\n",
       "      <td>4.521287e-12</td>\n",
       "      <td>4.356649e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>275.470643</td>\n",
       "      <td>334.023614</td>\n",
       "      <td>330.292063</td>\n",
       "      <td>333.180051</td>\n",
       "      <td>317.824092</td>\n",
       "      <td>306.451798</td>\n",
       "      <td>309.091647</td>\n",
       "      <td>312.060959</td>\n",
       "      <td>320.208051</td>\n",
       "      <td>336.027107</td>\n",
       "      <td>...</td>\n",
       "      <td>2.891349e-12</td>\n",
       "      <td>2.935824e-12</td>\n",
       "      <td>2.976514e-12</td>\n",
       "      <td>3.009965e-12</td>\n",
       "      <td>3.033863e-12</td>\n",
       "      <td>3.040662e-12</td>\n",
       "      <td>3.016554e-12</td>\n",
       "      <td>2.948046e-12</td>\n",
       "      <td>2.844024e-12</td>\n",
       "      <td>2.748188e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>319.154096</td>\n",
       "      <td>314.324236</td>\n",
       "      <td>322.541381</td>\n",
       "      <td>324.154848</td>\n",
       "      <td>333.263850</td>\n",
       "      <td>344.199198</td>\n",
       "      <td>338.490892</td>\n",
       "      <td>334.185374</td>\n",
       "      <td>327.065180</td>\n",
       "      <td>303.761845</td>\n",
       "      <td>...</td>\n",
       "      <td>1.335647e-12</td>\n",
       "      <td>1.361978e-12</td>\n",
       "      <td>1.387258e-12</td>\n",
       "      <td>1.409588e-12</td>\n",
       "      <td>1.427626e-12</td>\n",
       "      <td>1.437648e-12</td>\n",
       "      <td>1.432962e-12</td>\n",
       "      <td>1.406923e-12</td>\n",
       "      <td>1.363502e-12</td>\n",
       "      <td>1.323517e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>368.613557</td>\n",
       "      <td>330.588867</td>\n",
       "      <td>323.002818</td>\n",
       "      <td>322.824169</td>\n",
       "      <td>336.505619</td>\n",
       "      <td>328.080745</td>\n",
       "      <td>325.609686</td>\n",
       "      <td>335.941694</td>\n",
       "      <td>328.415271</td>\n",
       "      <td>330.880143</td>\n",
       "      <td>...</td>\n",
       "      <td>2.861867e-12</td>\n",
       "      <td>2.906931e-12</td>\n",
       "      <td>2.947792e-12</td>\n",
       "      <td>2.981218e-12</td>\n",
       "      <td>3.005087e-12</td>\n",
       "      <td>3.012002e-12</td>\n",
       "      <td>2.988298e-12</td>\n",
       "      <td>2.920602e-12</td>\n",
       "      <td>2.817710e-12</td>\n",
       "      <td>2.722916e-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      layer 1     layer 2     layer 3     layer 4     layer 5     layer 6  \\\n",
       "0  312.355087  301.649641  321.270850  324.903956  329.245296  337.694194   \n",
       "1  308.045992  314.390077  319.612686  324.021871  328.286597  324.268581   \n",
       "2  275.470643  334.023614  330.292063  333.180051  317.824092  306.451798   \n",
       "3  319.154096  314.324236  322.541381  324.154848  333.263850  344.199198   \n",
       "4  368.613557  330.588867  323.002818  322.824169  336.505619  328.080745   \n",
       "\n",
       "      layer 7     layer 8     layer 9    layer 10  ...      0.000008  \\\n",
       "0  333.776774  333.750946  331.605971  333.828275  ...  2.378753e-12   \n",
       "1  318.045216  310.205743  316.395558  315.206667  ...  4.690686e-12   \n",
       "2  309.091647  312.060959  320.208051  336.027107  ...  2.891349e-12   \n",
       "3  338.490892  334.185374  327.065180  303.761845  ...  1.335647e-12   \n",
       "4  325.609686  335.941694  328.415271  330.880143  ...  2.861867e-12   \n",
       "\n",
       "       0.000008      0.000008      0.000008      0.000008      0.000008  \\\n",
       "0  2.416740e-12  2.452565e-12  2.482976e-12  2.505710e-12  2.514345e-12   \n",
       "1  4.758857e-12  4.814785e-12  4.855500e-12  4.879557e-12  4.876002e-12   \n",
       "2  2.935824e-12  2.976514e-12  3.009965e-12  3.033863e-12  3.040662e-12   \n",
       "3  1.361978e-12  1.387258e-12  1.409588e-12  1.427626e-12  1.437648e-12   \n",
       "4  2.906931e-12  2.947792e-12  2.981218e-12  3.005087e-12  3.012002e-12   \n",
       "\n",
       "       0.000008      0.000008      0.000008      0.000008  \n",
       "0  2.497372e-12  2.443515e-12  2.360022e-12  2.283099e-12  \n",
       "1  4.823191e-12  4.700047e-12  4.521287e-12  4.356649e-12  \n",
       "2  3.016554e-12  2.948046e-12  2.844024e-12  2.748188e-12  \n",
       "3  1.432962e-12  1.406923e-12  1.363502e-12  1.323517e-12  \n",
       "4  2.988298e-12  2.920602e-12  2.817710e-12  2.722916e-12  \n",
       "\n",
       "[5 rows x 77 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_data = pd.read_excel(\"../UNT-Thermography-24/fused_silica_data/fused_silica_exact_train.xlsx\")\n",
    "\n",
    "exact_df = pd.DataFrame(exact_data)\n",
    "\n",
    "X = exact_df[exact_df.columns[11:]]\n",
    "\n",
    "Y = exact_df[exact_df.columns[0:11]]\n",
    "\n",
    "print(f\"The original data set has {exact_df.shape[0]} rows, and {exact_df.shape[1]} columns. \"\n",
    "      f\"We want to predict {target.shape[1]} output variables from {features.shape[1]} features\")\n",
    "\n",
    "exact_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize features\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 6400 samples for training, and 2000 for testing from 8400 total samples.\n"
     ]
    }
   ],
   "source": [
    "#Goal: Using keras to implement a feed-forward neural network to predict temperatures of each layer given wavelength spectrum\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled_df, Y, train_size = .8, random_state = 1337)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, train_size=0.8, random_state = 1337)\n",
    "\n",
    "print(\n",
    "    f\"Using {len(Y_train)} samples for training, \"\n",
    "    f\"and {len(Y_test)} for testing \"\n",
    "    f\"from {len(X_test) + len(X_train)} total samples.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 32274.5566 - mae: 141.7397 - val_loss: 172.9228 - val_mae: 9.5949\n",
      "Epoch 2/15\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 186.8254 - mae: 10.0881 - val_loss: 157.2077 - val_mae: 9.0459\n",
      "Epoch 3/15\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 210.9210 - mae: 10.4375 - val_loss: 206.3429 - val_mae: 10.8479\n",
      "Epoch 4/15\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 163.7638 - mae: 9.2648 - val_loss: 226.1487 - val_mae: 11.6138\n",
      "Epoch 5/15\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 175.4882 - mae: 9.5281 - val_loss: 155.7673 - val_mae: 8.5640\n",
      "Epoch 6/15\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 232.9620 - mae: 10.2287 - val_loss: 158.7628 - val_mae: 9.3306\n",
      "Epoch 7/15\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 141.8321 - mae: 8.1919 - val_loss: 185.9385 - val_mae: 10.2223\n",
      "Epoch 8/15\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 137.3368 - mae: 7.9178 - val_loss: 131.7912 - val_mae: 7.6719\n",
      "Epoch 9/15\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 132.0288 - mae: 7.6950 - val_loss: 136.4888 - val_mae: 7.9012\n",
      "Epoch 10/15\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 133.8600 - mae: 7.8266 - val_loss: 147.6174 - val_mae: 8.4152\n",
      "Epoch 11/15\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 134.1208 - mae: 7.8327 - val_loss: 147.0832 - val_mae: 7.7430\n",
      "Epoch 12/15\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 216.0011 - mae: 9.8570 - val_loss: 131.7759 - val_mae: 7.6575\n",
      "Epoch 13/15\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 155.5639 - mae: 8.7359 - val_loss: 153.4044 - val_mae: 8.5873\n",
      "Epoch 14/15\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 141.7647 - mae: 8.1516 - val_loss: 132.6941 - val_mae: 7.6060\n",
      "Epoch 15/15\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 143.6273 - mae: 8.2997 - val_loss: 139.4289 - val_mae: 7.9847\n",
      "train_MSE: [14120.7978515625, 189.64601135253906, 254.30494689941406, 167.4388427734375, 186.45408630371094, 383.58905029296875, 138.52938842773438, 133.99917602539062, 132.2181854248047, 135.35678100585938, 133.09365844726562, 265.5127868652344, 155.95376586914062, 135.7724609375, 146.4994659423828]\n",
      "train_MAE: [76.2113037109375, 10.182252883911133, 11.482982635498047, 9.387332916259766, 9.731629371643066, 13.197052955627441, 8.029067039489746, 7.779367446899414, 7.7178168296813965, 7.872076988220215, 7.76235294342041, 11.099976539611816, 8.763629913330078, 7.878696918487549, 8.412337303161621]\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 141.4175 - mae: 8.0037\n",
      "val_MSE: 139.42886352539062, val_MAE: 7.984687328338623\n"
     ]
    }
   ],
   "source": [
    "# Define the input shape\n",
    "input_layer = keras.Input(shape=(X_train.shape[1],))\n",
    "\n",
    "#Define the model (add more layers? increase nodes?)\n",
    "exact_model = keras.Sequential([input_layer, #input_layer --> informs input shape\n",
    "    layers.Dense(256, activation=\"relu\"), # first hidden layer\n",
    "    layers.Dense(128, activation=\"relu\"), # second hidden layer\n",
    "    # layers.Dense(20, activation=\"relu\"), # third hidden layer\n",
    "    # layers.Dense(20, activation=\"relu\"), # fourth hidden layer\n",
    "    layers.Dense(11)  # Output layer with 11 neurons for each temperature target\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "# Instantiate optimizer with adjustable learning rate\n",
    "# observed: sweet spot .02 > x > .001\n",
    "Adam1 = Adam(learning_rate = 0.009)\n",
    "\n",
    "# Leverage both MSE and MAE (experimental)\n",
    "# test different optimizers: SGD (*stochastic gradient descent: better for large data sets)\n",
    "exact_model.compile(loss=\"mse\", optimizer=Adam1, metrics=[\"mae\"])  # Loss: MSE, Optimizer: Adam w/ MAE\n",
    "\n",
    "# Train the model\n",
    "# Thoughts: Overfit then implement regularization\n",
    "# play around with batch size: batch size for larger datasets more likely to improve its performance\n",
    "# Dima went up to 60 epochs for GaN_exact: later 600 -> imrpovement plateaus around 120 epochs\n",
    "# plot loss over epochs\n",
    "history = exact_model.fit(X_train, Y_train, epochs=15, validation_data = [X_val, Y_val])\n",
    "\n",
    "#View Training Error\n",
    "print(f\"train_MSE: {history.history[\"loss\"]}\")\n",
    "print(f\"train_MAE: {history.history.get(\"mae\", None)}\")\n",
    "\n",
    "# Evaluate the model on the validation set (separate call)\n",
    "validation_loss, validation_accuracy = exact_model.evaluate(X_val, Y_val)\n",
    "\n",
    "print(f\"val_MSE: {validation_loss}, val_MAE: {validation_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 122.8408 - mae: 7.3795\n",
      "test_MSE: 122.12621307373047\n",
      "test_MAE: 7.383547306060791\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on unseen data\n",
    "loss, mae = exact_model.evaluate(X_test, Y_test)\n",
    "print(\"test_MSE:\", loss)\n",
    "print(\"test_MAE:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "#plot predictions against actual (scatter plot to see if similar)\n",
    "#m nodes by n layers: want m*n to be less than number of data points\n",
    "#per layer residual\n",
    "#avg (above) to get overall residual\n",
    "#plot error over epochs\n",
    "#increase layers: implement loop\n",
    "#change activation function to hyperbolic tangent: (theoretical difference: ReLu 0 and 1, tangent maintains positive and negative (if normalized data is positive and negative, it is possible tanegent would work better (consider what might happen if all data is positive)))\n",
    "# 20 layers 250 nodes, batch size 15, epochs 220\n",
    "#learning rate vs momenta/momento?? may reduce epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
